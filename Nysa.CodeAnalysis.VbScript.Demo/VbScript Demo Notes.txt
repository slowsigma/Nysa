 <Page 01>
 1. Our story starts at the birth of Tyler Technologies' Enterprise Justice Suite, or the product formerly known as Odyssey.
    Back around the turn of the century (ha, ha), the Odyssey development team recognized the potential of internet technology
    but they made the fateful decision to adopt an all Microsoft tech stack. It was not a bad decision at the time, just the
    wrong one. That decision meant coding UI logic with the now defunct VB Script language. Even though the dev community at
    large would recognized JavaScript as the winner much earlier, the demise of VB Script was sealed in late 2018 when
    Microsoft announced it would base it's new Edge browser on the same technology as the Google's Chrome browser. So, we
    should have started turning the Titanic even before that happened, but we didn't. By the time we started the turn, we had
    a full two decades of "investment" to fix (over 2.1 million lines of VB Script).  There are many ways of calculating the
    worth of a product like Enterprise Justice. You can use future profit, past profit, both, or you can calculate the total
    amount of money poured into its existence. For this demo, and my click-bait title here, I chose calculating value based on
    cost. Does anyone know the historical average lines of code per day a developer produces? What about the number of bugs
    created per thousand lines of code?
<Page 02>
 2. For the sake of perspective, this page shows a best case scenario of how to potentially solve the problem by throwing
    man power at it. There are trade offs to be made here. If you slow down to reduce the number of bugs, you reduce the
    number of lines of code per day. The bottom line is that a manual effort introduces huge risks. Of course, we did
    successfully turn the Titanic around and there are many dimensions to that story, but this talk is about the translation
    tool we created and the technology under the hood. But first, I have to take a moment to point out that this project
    originated as a hobby project and that means the final translator we created is based on open source code I created on
    my own. For this demo, I decided it was going to be open source as well and that mean it does not use, or show, any
    Tyler owned code.
<Page 03>
 3. This page shows the repo containing this demo (its a window app) and the two primary class libraries the translator and
    this demo use. This talk will mainly be about concepts and not code and I will point out when we jump from the concepts
    contained in the open source portion to that of the Tyler translator.
<Page 04>
 4. So, here's my less glamorous title for this talk.
 5. Its easy to see code as unstructured like natural language and, therefore, not data, but we can't translate code in any
    predictable way unless we can turn it into data.
<Page 05>
    The basic idea of code translation is: turning code into data, and then turning that data back into code.
<Page 06>
 6. We've actually been doing this since the very first high-level programming language.
 7. Compilers read code and turn that into a string of machine code.
 8. The whole idea for the translator was inspired by my exposure to Microsoft class libraries that came from their Roslyn
    project for .NET.
<Page 07>
 9. So, when it comes to compilers, we have some core terms that serve as "stepping stones" to understanding what compilers
    do. These will also serve as chronological markers in the timeline of making the translator.  Some of these terms may be
    familiar and some may not, but by the end, you should have a pretty good understanding of each.
10. Let's start with source code.
<Page 08>
11. I wrote this little sample script as a starting point and it will feed into different parts of the presentation as we
    go along. The program simply prints out a random German greeting.
<Page 09>
12. Back in late 2017, while looking at the Roslyn libraries, I got this idea that we might be able to translate the
    Enterprise Justice VB Script to JavaScript if we could parse the source into a semantic tree.  A semantic tree is
    simply a direct representation of source code in a tree data structure in memory (much like parsed XML has a tree
    data structure).
13. The first thing I needed was the exact grammar rules for VB Script. So, I did some online searching and was able to
    find an attempt some developers had made at writing down the rules. There were some errors in the rules, but it
    served as a good starting point.
<Page 10>
14. Here are those rules in a language called Backus-Naur Form (BNF). Its an extremely simple language to follow.
    Each line contains a single rule.  Each rule starts with an abstract symbol (that's the thing between greater and
    less-than characters), followed by the colon-colon-equals symbol, which stands for "is defined as", and followed
    by a set of alternate definitions separated by the vertical bar (or pipe character). Each alternate definition
    is simply a sequence of literals (the quoted strings), abstract symbols (< >), and literal categories (a name
    surrounded by curly braces).
15. Of course, the difficulty with this grammar rule language is that you don't really get to see how it works when
    you look at it raw like this. So, I've applied a little logic to it and turned it into an interactive page.
<Page 11>
16. In a BNF document for a programming language, there's always a start rule and, of course, that's the rule that
    no other rule references. So, its fitting that the start rule uses the word "program" because that's what we
    get when we write code in this language (we get a program).
    This little page shows how the rules expand downward as they are used in a real piece of code.
    {Work the page expanding terms...}
<Page 12>
17. Here's an example of that same grammar in C# code. This shows an API that I built to allow a developer to take
    grammar rules and turn them into a complete grammar object which feeds into the rest of the process. Very rarely
    is a BNF document consumed by a program that would then directly compile the code it describes. Instead, it's
    typically used by a program that generates the parsing code of a compiler.
<Page 13>
18. From the grammar, and some additional definitions for the literal categories, the API I created builds a lexical
    analyzer (or lexer) which takes source code and turns it into a stream of tokens (each token is a single word in
    the language of the grammar).
<Page 14>
19. Here we have the sample code on your left and the token stream produced by the Nysa.CodeAnalysis library on the
    right. One thing to notice is that this lexical output does not keep comments, which, in the Microsoft Roslyn
    API is refered to as trivia.
    (add comments to the sample source)
    (separate out the combined dim statement)
    You'll notice that the lexing logic does not care if I break the program.  That happens in the parsing logic.
<Page 15>
20. And here I'm flipping the content around to show the detail provided by lexical analysis and to demonstrate
    how lexing provides code colorization.
    Notice how the word 'explicit' gets the same identifier as our variable names and a unique identifier, but
    that 'option' just has a unique value.  This is all due to the grammar rules.
    (arrow down to a liteal string)
    Notice that a literal string is a single token.
<Page 16>
21. With a token stream from the lexical analyzer and the grammar rules, we can actually perform the parsing.
    The logic I used for this part of the API came from Wikipedia. I decided to use what's called the Earley
    parsing algorithm primarily because its very permissive when it comes to how the grammar rules are written.
    Some parsing algorithms have strict limitations on how the grammar rules are structured.  I'm not going to
    dive into that any further here.  You can read about that on your own if you're interested.  The end result
    of most parsing algorithms is to get what's called an abstract syntax tree (or AST).  The Earley algorithm,
    however, does not produce an AST by itself, it produces a parse chart that needs some additional processing
    in order to get the AST.
<Page 17>
22. I've designed this page to show the Earley algorithm at work. You can step through token-by-token as the
    algorithm charts out if, and how, the input tokens satisfy the rules of the grammar.
    I'll step through several cycles and describe some of what's happening. We start with an empty chart
    which, obviously, not much to see yet. What we do have here is a list of the tokens on the left
    along with the number of rules the algorithm has put into the chart in its attempt to match that
    particular token in the grammar.
    (hit step once)
    The first step in the algorithm is to add all the alternatives of the start rule into the chart.
    For VB Script, that's the "program" rule and it only has one alternative. One thing to note here is
    that we slightly changed the definition of what a rule is here. When we looked at the syntax rules
    before, each rule consisted of all the alternatives. Here we treat each alternative in a syntax rule
    as an independent rule entry in the chart.
    Each entry has a progress marker which is shown here as this little spikey circle character right
    after the colon-colon-equals. This shows no progress yet for this entry.
    (hit fast forward)
    (scroll down and highlight the last chart position)
    I've skipped to the end to get the fully populated chart. At this point we know this program satisfies
    the grammar because we have the "program" rule carried forward to the end and its progress indicator is
    set past the last symbol of the rule.
    We apply another algorithm to this chart to produce the abstract syntax tree. I had a pretty hard time
    finding a good explanation of exactly how to do that, but I was able to piece it together from some
    internet sources and a lot of trial and error.
    I'll let you restless souls that just can't stand not knowing dig into the git repo yourselves, but
    I will show the one step that's taken to prepare this chart to get the AST and its called inverting the
    chart.
    (click the inverse checkbox) 
    As you can see here we've created a kind of mirror chart where all the entries are in
    reverse order and the rule start positions (the number in those brackets) are changed to
    reflect where that rule came from before the flip.  One other thing that's happened here
    is that we've removed all the unsatisfied entries in the chart.



<Page 18>
    That's what feeds the algorithm to get the abstract syntax tree (the AST).
<Page 19>
    Here's that AST on the left and our source on the right. This tree is simply composed of
    objects of four main types. There's a Node type, which has a property pointing to the
    rule the node represents, and a property that lists all its children. The type that the
    list is made of is literally NodeOrToken. Each NodeOrToken object has a nullable token
    property and a nullable node poperty, only one of which will be null when the other is
    not.
    (expand and select different parts of the tree)




    If the code does not violate any of the syntax rules, the chart will end up being the number of
    input tokens plus one and each position of the chart will have at least one entry.
    
    The algorithm will continue as long there are still tokens to match and there are rules in chart it can use to match.


    So, obviously, the algorithm is incrementing through the input tokens one at a time, but it's also
    adding to the chart at each token position, and it's analyzing each of these chart entries as it goes.
    Each chart entry is a copy of a single alternative of a syntax rule and a progress marker indicating
    how much of that variation has been matched up to that point. Remember that the alternatives in our
    syntax rules were the individual sequences separated by the vertical bars.  We see the the start rule
    here, which has only one alternative, and the progress indicator is this little symbol right after the
    colon-colon-equals.  I'm using the character for generic currency here.
    (hit step once and the go back to chart entry zero of token zero)
    So, here, you're seeing that the algorithm added a bunch of entries under the very first token with
    the progress indicator at various locations within these entries.
    (highlight the "<OptionExplicit>" entry at position 20)
    The algorithm makes entries in the next token position when it matches a current entry to the current
    token, or it finds a completed rule in the chart at the current position and it finds a symbol match
    for that completed rule in a prior chart entry. Here we're seeing the chart entry that matches this
    this "option" token.
    (hit step once and go back to token position 1)
    This shows that the only rule that can match at token position 1 is the <OptionExplicit> rule.
    (go to token position 1)
    The entries here represent both the token match and an entry that the algorithm placed here just
    after that token match which essentially represents the next symbol after the token match that
    the algorithm can accept at position 2.

    
    As the algorithm is running, the rules are being appended to the end of the list
    of rules at the current token position and, potentially, to the list of rules in the next token
    position.
    Each entry in the chart is a rule and its parsing progress at that point.
    In the algorithm, there are two logic paths for a rule to progress down in the chart. First, if the
    next available symbol of the rule is a token and it matches the current token in the input stream,
    then the rule moved to the next chart position and the progress marker is advanced. Second, when the
    algorithm sees that a rule has been advanced and it's actually completed (the progress marker is past
    the last symbol of the rule), the algorithm search back in the chart for prior incomplete rules where
    the next available symbol in the rule matches the symbol of this completed rule.




































<<< TAKEN OUT OF THE PARSE CHART NOTES >>>
    So, the right side of this page is showing you the list
    within the list for the token we have selected on the left. Each chart entry has a progress indicator
    showing how much of the rule is matched at this point. The progress indicator is that little spikey
    circle character right after the colon-colon-equals and its showing here that nothing has been
    matched yet. In addition, each entry contains the token index at which the entry was first created.
    Here you can see the number in the brackets just prior to the colon-colon-equals.
    (hit step once)
    So after another step, this is showing the starting point of the chart for the next token. You can
    see that we ended up with one hundred six entries created in the chart at token zero just to get to
    the starting point of token one. We'll look back at what happened.
    (select token zero and scroll to entry zero)
    Before we dig into the details of these entries, its important to understand that the algorithm
    does not use for each loops.  This is a dynamic list of lists and the algorithm is doing a kind of
    loop like "while there are entries at the next position, increment to the next position and
    perform logic that might add more to this list or the next list". With that in mind, let's look
    at what was added after our very first entry. The algorithm is standing on entry zero and it
    performs some action based on what the next thing is after the progress indicator. Here, the
    next thing is the abstract symbol "NLOpt". So, the algorithm looks up the alternative rules
    that make up "NLOpt" and adds each to the list. In addition, because the "NLOpt" rule is, by
    its defintion, and optional rule, the algorithm determines that it can take the rule its on,
    the "program" rule, and add it to the end of the list with the progress marker advanced over
    the "NLOpt" symbol. Done with that, the algorithm advances to entry one and evaluates the next
    symbol in that rule. Here its the "NL" rule with its four alternatives. This process continues
    the same until we get to entry four where the algorithm encounters an entry where the next
    thing to match is an actual token. In this case, its the token category "new-line". This is a
    category because the lexing algorithm will make a single token for a carriage return character,
    or a line feed character, or combination of the two. Of course, the algorithm checks this token
    against the current token and it doesn't match, so it moves on to the next chart entry. Things
    progress in this same way until we get to entry twenty where you can see the next thing to
    match is the "option" token which is a match for the current token and this is where the
    algorithm does something entirely different. It adds a copy of the current rule, with the
    progress indicator advanced to the next symbol, into the list of entries corresponding to the
    next token.
    (select entry twenty, pause)
    (select token one)
    One other thing to note here is that since we only see this one entry at token one, it means
    that after entry twenty, all the rest of the evaulation beyond that point resulted in no
    additional match for the "option" token. This makes sense for VB Script, but it does imply that
    for some languages a single keyword may apply to more than one rule. Notice that the next thing
    to match here is the token "explicit". This means that there will be no abstract rule expansion
    in this list and if the current token does not match, parsing will be done resulting in a
    syntax error. There's still one more behavior of the algorithm to talk about and so, I'm going
    to step forward a few more times in order to show it.
    (hit step four times and then select token two)
    Notice here that the "option explicit" rule was pushed forward once more from a match on the
    "explicit" token and still has the "NL" rule as the next thing to match.
    (select token three and scroll down to entry zero)
    Here at index zero and one we have two rules that were pushed forward from the "new-line"
    match at index 2, one of these needs another "NL" and the other is complete. The next entries,
    two through five come from the expansion of "NL" at index zero. When the algorithm gets to
    entry one, it does something different. Here the algorithm searches back through the chart
    for entries where the next symbol needed is an "NL" and of course, it find that in the
    "option explicit" chart entry at index two.
    (highlight token two)
    This "option explicit" rule here gets pulled into the chart list at token position three and
    its progress marker is advanced. In turn, when the algorithm encounters the "option explicit"
    that completed here, it searches back in the chart for a rule whose next match is this rule.
    You can see that's the reason for the "global stmt" entry at position nine. This rule pulling
    behavior also accounts for the "global stmt list" entry at ten and the "program" entry at
    twenty one.
    That covers the major behaviors of the algorithm, but its worth pointing out one small thing
    I skipped over before now. Notice this token category just after the progress marker on the
    "program" entry here at position twenty one.  That is a special token which the lexer appends
    at the end of the token stream.
    (scroll down the token list to show the EOI)
    This means that the "program" rule is never satisfied until the end of the token stream has
    been processed. I'll press the fast forward button here to get the algorithm to finish up the
    chart.
    (press fast forward and highlight the last token)
    Because we have a final satisfied start symbol at the end of the chart, we say that the
    program code satisfies the syntax rules our VB Script grammar.
    That's the Earley algorithm in a nutshell and it produces this chart to which we apply
    another algorithm to produce the abstract syntax tree. I had a pretty hard time finding a
    good explanation of exactly how to do that, but I was able to piece it together from some
    internet sources and a lot of trial and error.
    I'll leave that algorithm for those restless souls that just can't stand not knowing, but
    I will show the one step that's taken to prepare this chart to get the AST and its called
    inverting the chart.
    (click the inverse checkbox) 
    As you can see here we've created a kind of mirror chart where all the entries are in
    reverse order and the rule start positions (the number in those brackets) are changed to
    reflect where that rule came from before the flip.  One other thing that's happened here
    is that we've removed all the unsatisfied entries in the chart.
