 <Page 01>
 1. Before we begin.
    This entire demo is open source.  It can be found here (show GitHub and NuGet locations).
    I'm only going to include material here that is open source. Yes, I am the author and owner of all the code and examples
    I'm going to show here. I will talk about portions of the project that were officially Tyler, but there is no code in this
    that is Tyler owned.

 2. Our story starts at the birth of Tyler Technologies' Enterprise Justice Suite, originally titled Odyssey.
    Back around the turn of the century (ha, ha), the Odyssey development team understood the potential of internet technology
    but they made the fateful decision to adopt an all Microsoft tech stack. It was not a bad decision at the time, just the
    wrong one as we now know. That decision meant coding UI logic with the now defunct VB Script language. Even though the dev
    community at large would recognized JavaScript as the winner much earlier, the demise of VB Script was sealed in late 2018
    when Microsoft announced it would base it's new Edge browser on the same technology as the Google's Chrome browser. So, we
    should have started turning the Titanic even before that happened, but we didn't. By the time we started the turn, we had
    a full two decades of "investment" to fix. There are many ways of calculating the worth of a product like Enterprise
    Justice. You can use future profit, past profit, both, or you can calculate the total amount of money poured into its
    existence. For this demo, and my click-bait title here, I chose calculating value based on cost (i.e., how much have we
    sunk into the beast).
    I believe this is a pretty conservative estimate and I've even added a factor here to attempt to make the number reflect
    the cost of the user interface alone.
<Page 02>
 3. The real title of this talk is really, "Translating VB Script to JavaScript".
    If you didn't catch it from the click bait title screen we just saw, this is not a static slide presentation.
    The entire thing is a program that contains slides and interactive content that is actually using the open source
    code that the official Tyler VB Script translator was built upon.
    The basic idea of code translation is: turning code into data, and then turning that data back into code.
<Page 03>


 4. We've actually been doing this since the very first high-level programming language.
<Page 04>
 5. Compilers read code and turn that into a string of machine code.
 6. This project was inspired by the Microsoft Roslyn project which I believe was created to enable all kinds of
    IDE features we take for granted today, primarily those features that allow us to jump back and forth in code
    between the definitions of symbols and the places where those symbols are used.
<Page 05>
 7. So, when it comes to compilers, there are some core terms that form the back bone of what this presentation covers.
    Some of these may be familiar and some may not, but by the end, you'll have a pretty good idea about how the
    translator works.
<Page 06>
 8. We start with source code.  I wrote this little sample script as a starting point and it will feed into different
    parts of the presentation as we go along.  The program simply prints out a random German greeting.
<Page 07>
 9. Back in late 2017, I got this idea that we might be able to translate the Enterprise Justice VB Script to JavaScript
    if we could parse the source into a semantic tree.  A semantic tree is simply a direct representation of source code
    in computer memory in a tree structure (much like parsed XML in memory has a tree structure).
10. The first thing I needed was the exact grammar rules for VB Script. So, I did some online searching and was able to
    find an attempt some developers had made at writing down the rules. There were some errors in the rules, but it
    served as a good starting point.
<Page 08>
11. Here are those rules in a language called Backus-Naur Form (BNF). Its an extremely simple language to follow.
    Each line contains a single rule.  Each rule starts with an abstract symbol (that's the thing between greater and
    less-than characters), followed by the colon-colon-equals symbol, which stands for "is defined as", and followed
    by a set of alternate definitions separated by the vertical bar (or pipe character). Each alternate definition
    is simply a sequence of literals (the quoted strings), abstract symbols (< >), and literal categories (a name
    surrounded by curly braces).
12. Of course, the difficulty with this grammar rule language is that you don't really get to see how it works when
    you look at it raw like this. So, I've applied a little logic to it and turned it into an interactive page.
<Page 09>
13. In a BNF document for a programming language, there's always a start rule and, of course, that's the rule that
    no other rule references. So, its fitting that the start rule uses the work "program" because that's what you
    end up with at the top level.
    This little page shows how the rules expand downward as they are used in a real piece of code.
    {Work the page expanding terms...}
<Page 10>
14. Here's an example of that same grammar in C# code. This shows an API that I built to allow a developer to take
    grammar rules and turn them into a complete grammar object which feeds into the rest of the process. Very rarely
    is a BNF document consumed by a program that would then directly compile the code it describes. Instead, it's
    typically used by a program that generates the parsing code of a compiler.
<Page 11>
15. From the grammar, and some additional definitions for the literal categories, the API I created builds a lexical
    analyzer (or lexer) which takes source code and turns it into a stream of tokens (each token is a single word in
    the language of the grammar).
<Page 12>
16. So, here we have the sample code on your left and the token stream produced by the Nysa.CodeAnalysis library on
    the right.
    One thing to notice is that this lexical output does not keep comments, which, in the Microsoft Roslyn API is
    refered to as trivia.
    (add comments to the sample source)
    (separate out the combined dim statement)
    You'll notice that the lexing logic does not care if I break the program.  That happens in the parsing logic.
<Page 13>
17. And here I'm flipping the content around to show the detail given by data produced from lexical analysis and
    to demonstrate how lexing provides code colorization.  Each token provides the category, or categories, the
    token matches and the token position and length in the source.
    Notice how the word 'explicit' gets the same identifier as our variable names and a unique identifier, but
    that 'option' just has a unique value.  This is all due to the grammar rules.
    (arrow down to a liteal string)
    Notice that a literal string is a sinle token.
<Page 14>
18. With a token stream from the lexical analyzer and the grammar rules, we can actually perform the parsing.
    The logic I used for this part of the API came from Wikipedia. I decided to use what's called the Earley
    parsing algorithm primarily because its very permissive when it comes to how the grammar rules are written.
    Some parsing algorithms have strict limitations on how the grammar rules are laid out. This mainly comes
    down to whether or not they are left-recursive or right-recursive, but I'm not going to dive into that any
    further here.  You can read about that on your own if you're interested.  The end result of any parsing
    algorithm is to get what's called an abstract syntax tree (or AST).  The Earley algorithm, however, does
    not produce an AST by itself, it produces a parse chart that needs some additional processing in order to
    get the AST.
<Page 15>
19. I've designed this page to show the Earley algorithm at work. You can step through token-by-token as the
    algorithm charts out if, and how, the input tokens satisfy the rules of the grammar.
    I'll step through several cycles and describe some of what's happening. We start with an empty chart
    which, obviously, not much to see yet. What we do have here is a list of the tokens on the left
    along with the number of rules the algorithm has put into the chart in its attempt to match that
    particular token in the grammar.
    (hit step once)
    The first step in the algorithm is to add all the alternatives of the start rule into the chart.
    For VB Script, that's the "program" rule and it only has one alternative. One thing to note here is
    that we slightly changed the definition of what a rule is here. When we looked at the syntax rules
    before, each rule consisted of all the alternatives. Here we treat each alternative in a syntax rule
    as an independent rule entry in the chart. So, the right side of this page is showing you the list
    within the list for the token we have selected on the left. Each chart entry has a progress indicator
    showing how much of the rule is matched at this point. The progress indicator is that little spikey
    circle character right after the colon-colon-equals and its showing here that nothing has been
    matched yet. In addition, each entry contains the token index at which the entry was first created.
    Here you can see the number in the brackets just prior to the colon-colon-equals.
    (hit step once)
    So after another step, this is showing the starting point of the chart for the next token. You can
    see that we ended up with one hundred six entries created in the chart at token zero just to get to
    the starting point of token one. We'll look back at what happened.
    (select token zero and scroll to entry zero)
    Before we dig into the details of these entries, its important to understand that the algorithm
    does not use for each loops.  This is a dynamic list of lists and the algorithm is doing a kind of
    loop like "while there are entries at the next position, increment to the next position and
    perform logic that might add more to this list or the next list". With that in mind, let's look
    at what was added after our very first entry. The algorithm is standing on entry zero and it
    performs some action based on what the next thing is after the progress indicator. Here, the
    next thing is the abstract symbol "NLOpt". So, the algorithm looks up the alternative rules
    that make up "NLOpt" and adds each to the list. In addition, because the "NLOpt" rule is, by
    its defintion, and optional rule, the algorithm determines that it can take the rule its on,
    the "program" rule, and add it to the end of the list with the progress marker advanced over
    the "NLOpt" symbol. Done with that, the algorithm advances to entry one and evaluates the next
    symbol in that rule. Here its the "NL" rule with its four alternatives. This process continues
    the same until we get to entry four where the algorithm encounters an entry where the next
    thing to match is an actual token. In this case, its the token category "new-line". This is a
    category because the lexing algorithm will make a single token for a carriage return character,
    or a line feed character, or combination of the two. Of course, the algorithm checks this token
    against the current token and it doesn't match, so it moves on to the next chart entry. Things
    progress in this same way until we get to entry twenty where you can see the next thing to
    match is the "option" token which is a match for the current token and this is where the
    algorithm does something entirely different. It adds a copy of the current rule, with the
    progress indicator advanced to the next symbol, into the list of entries corresponding to the
    next token.
    (select entry twenty, pause)
    (select token one)
    One other thing to note here is that since we only see this one entry at token one, it means
    that after entry twenty, all the rest of the evaulation beyond that point resulted in no
    additional match for the "option" token. This makes sense for VB Script, but it does imply that
    for some languages a single keyword may apply to more than one rule. Notice that the next thing
    to match here is the token "explicit". This means that there will be no abstract rule expansion
    in this list and if the current token does not match, parsing will be done resulting in a
    syntax error. There's still one more behavior of the algorithm to talk about and so, I'm going
    to step forward a few more times in order to show it.
    (hit step four times and then select token two)
    Notice here that the "option explicit" rule was pushed forward once more from a match on the
    "explicit" token and still has the "NL" rule as the next thing to match.
    (select token three and scroll down to entry zero)
    Here at index zero and one we have two rules that were pushed forward from the "new-line"
    match at index 2, one of these needs another "NL" and the other is complete. The next entries,
    two through five come from the expansion of "NL" at index zero. When the algorithm gets to
    entry one, it does something different. Here the algorithm searches back through the chart
    for entries where the next symbol needed is an "NL" and of course, it find that in the
    "option explicit" chart entry at index two.
    (highlight token two)
    This "option explicit" rule here gets pulled into the chart list at token position three and
    its progress marker is advanced. In turn, when the algorithm encounters the "option explicit"
    that completed here, it searches back in the chart for a rule whose next match is this rule.
    You can see that's the reason for the "global stmt" entry at position nine. This rule pulling
    behavior also accounts for the "global stmt list" entry at ten and the "program" entry at
    twenty one.
    That covers the major behaviors of the algorithm, but its worth pointing out one small thing
    I skipped over before now. Notice this token category just after the progress marker on the
    "program" entry here at position twenty one.  That is a special token which the lexer appends
    at the end of the token stream.
    (scroll down the token list to show the EOI)
    This means that the "program" rule is never satisfied until the end of the token stream has
    been processed. I'll press the fast forward button here to get the algorithm to finish up the
    chart.
    (press fast forward and highlight the last token)
    Because we have a final satisfied start symbol at the end of the chart, we say that the
    program code satisfies the syntax rules our VB Script grammar.
    That's the Earley algorithm in a nutshell and it produces this chart to which we apply
    another algorithm to produce the abstract syntax tree. I had a pretty hard time finding a
    good explanation of exactly how to do that, but I was able to piece it together from some
    internet sources and a lot of trial and error.
    I'll leave that algorithm for those restless souls that just can't stand not knowing, but
    I will show the one step that's taken to prepare this chart to get the AST and its called
    inverting the chart.
    (click the inverse checkbox) 
    As you can see here we've created a kind of mirror chart where all the entries are in
    reverse order and the rule start positions (the number in those brackets) are changed to
    reflect where that rule came from before the flip.  One other thing that's happened here
    is that we've removed all the unsatisfied entries in the chart.
<Page 16>
    That's what feeds the algorithm to get the abstract syntax tree (the AST).
<Page 17>
    Here's that AST on the left and our source on the right. This tree is simply composed of
    objects of four main types. There's a Node type, which has a property pointing to the
    rule the node represents, and a property that lists all its children. The type that the
    list is made of is literally NodeOrToken. Each NodeOrToken object has a nullable token
    property and a nullable node poperty, only one of which will be null when the other is
    not.
    (expand and select different parts of the tree)




    If the code does not violate any of the syntax rules, the chart will end up being the number of
    input tokens plus one and each position of the chart will have at least one entry.
    
    The algorithm will continue as long there are still tokens to match and there are rules in chart it can use to match.


    So, obviously, the algorithm is incrementing through the input tokens one at a time, but it's also
    adding to the chart at each token position, and it's analyzing each of these chart entries as it goes.
    Each chart entry is a copy of a single alternative of a syntax rule and a progress marker indicating
    how much of that variation has been matched up to that point. Remember that the alternatives in our
    syntax rules were the individual sequences separated by the vertical bars.  We see the the start rule
    here, which has only one alternative, and the progress indicator is this little symbol right after the
    colon-colon-equals.  I'm using the character for generic currency here.
    (hit step once and the go back to chart entry zero of token zero)
    So, here, you're seeing that the algorithm added a bunch of entries under the very first token with
    the progress indicator at various locations within these entries.
    (highlight the "<OptionExplicit>" entry at position 20)
    The algorithm makes entries in the next token position when it matches a current entry to the current
    token, or it finds a completed rule in the chart at the current position and it finds a symbol match
    for that completed rule in a prior chart entry. Here we're seeing the chart entry that matches this
    this "option" token.
    (hit step once and go back to token position 1)
    This shows that the only rule that can match at token position 1 is the <OptionExplicit> rule.
    (go to token position 1)
    The entries here represent both the token match and an entry that the algorithm placed here just
    after that token match which essentially represents the next symbol after the token match that
    the algorithm can accept at position 2.

    
    As the algorithm is running, the rules are being appended to the end of the list
    of rules at the current token position and, potentially, to the list of rules in the next token
    position.
    Each entry in the chart is a rule and its parsing progress at that point.
    In the algorithm, there are two logic paths for a rule to progress down in the chart. First, if the
    next available symbol of the rule is a token and it matches the current token in the input stream,
    then the rule moved to the next chart position and the progress marker is advanced. Second, when the
    algorithm sees that a rule has been advanced and it's actually completed (the progress marker is past
    the last symbol of the rule), the algorithm search back in the chart for prior incomplete rules where
    the next available symbol in the rule matches the symbol of this completed rule.






